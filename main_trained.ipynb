{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d--cuzlQ_Zaj",
        "outputId": "4d6b55ba-c86b-4c0b-c53e-cb4a40b89a30"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import AutoTokenizer\n",
        "import nltk\n",
        "# from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXGpzBRn_Zal",
        "outputId": "2bee4128-690c-404a-bdd6-5691da99e4af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"TITLE: Alice's Adventures in Wonderland\\n\",\n",
              " 'AUTHOR: Lewis Carroll\\n',\n",
              " '\\n',\n",
              " '\\n',\n",
              " '= CHAPTER I = \\n',\n",
              " '=( Down the Rabbit-Hole )=\\n',\n",
              " '\\n',\n",
              " '  Alice was beginning to get very tired of sitting by her sister\\n',\n",
              " 'on the bank, and of having nothing to do:  once or twice she had\\n',\n",
              " 'peeped into the book her sister was reading, but it had no\\n']"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# df = pd.read_csv('/content/alice_in_wonderland.txt')\n",
        "# mytext = df['question1'].tolist()\n",
        "# mytext[:5]\n",
        "\n",
        "text = ''\n",
        "\n",
        "with open('/content/alice_in_wonderland.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.readlines()\n",
        "\n",
        "text[:10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "J-5JHAHM_Zal"
      },
      "outputs": [],
      "source": [
        "max_seq_len = 512\n",
        "vocab_size = 30000\n",
        "embed_dim = 512\n",
        "num_layers = 6\n",
        "num_heads = 8\n",
        "hidden_dim = 1024\n",
        "batch_size = 16\n",
        "epochs = 10\n",
        "learning_rate = 1e-4\n",
        "warmup_steps = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "yAxPRQNM_Zam"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_len=max_seq_len):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        encoding = self.tokenizer.encode(text, max_length=self.max_len, padding='max_length', truncation=True)\n",
        "        return torch.tensor(encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "P0kjshaO_Zam"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_layers, num_heads, hidden_dim, max_seq_len):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.positional_encoding = self.get_positional_encoding(max_seq_len, embed_dim)\n",
        "\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
        "\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        device = x.device\n",
        "        positional_encoding = self.positional_encoding.to(device)\n",
        "        # x = self.embedding(x) + self.positional_encoding[:x.size(1), :]\n",
        "        x = self.embedding(x) + positional_encoding[:, :x.size(1), :]\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = self.fc(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "    def get_positional_encoding(self, max_seq_len, embed_dim):\n",
        "        pe = torch.zeros(max_seq_len, embed_dim)\n",
        "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-np.log(10000.0) / embed_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        return pe.unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc-KqHDN_Zam",
        "outputId": "87851211-43e3-4df5-eb73-0c622cae810a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "texts = text\n",
        "# tokenizer = word_tokenize\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "train_texts, val_texts = train_test_split(texts, test_size=0.2)\n",
        "\n",
        "train_dataset = TextDataset(train_texts, tokenizer)\n",
        "val_dataset = TextDataset(val_texts, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model = TransformerModel(vocab_size=vocab_size, embed_dim=embed_dim, num_layers=num_layers,\n",
        "                         num_heads=num_heads, hidden_dim=hidden_dim, max_seq_len=max_seq_len)\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=epochs*len(train_loader))\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "EeCDRThf_Zan"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch.long().to('cuda')\n",
        "            outputs = model(input_ids)\n",
        "\n",
        "            outputs = outputs.view(-1, vocab_size)\n",
        "            input_ids = input_ids.view(-1)\n",
        "            loss = criterion(outputs, input_ids)\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "G7aHX0SJ_Zao"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids =batch.long().to('cuda')\n",
        "            outputs = model(input_ids)\n",
        "\n",
        "            outputs =outputs.view(-1, vocab_size)\n",
        "            input_ids = input_ids.view(-1)\n",
        "            loss = criterion(outputs, input_ids)\n",
        "            total_loss +=loss.item()\n",
        "    return total_loss/len(val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3qUNLZJ_Zao",
        "outputId": "782c3163-145f-45e1-d1c1-ed2fc15f23f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 3.777030020621088\n",
            "Epoch 2/10, Loss: 0.1688496134347386\n",
            "Epoch 3/10, Loss: 0.09224488430966933\n",
            "Epoch 4/10, Loss: 0.05929569707562526\n",
            "Epoch 5/10, Loss: 0.04025054987933901\n",
            "Epoch 6/10, Loss: 0.028903769831069643\n",
            "Epoch 7/10, Loss: 0.021493608540751868\n",
            "Epoch 8/10, Loss: 0.017570449577437506\n",
            "Epoch 9/10, Loss: 0.015315832109707925\n",
            "Epoch 10/10, Loss: 0.014215607268528805\n",
            "Validation Loss:0.018477487978008057\n"
          ]
        }
      ],
      "source": [
        "# model.to('cpu')\n",
        "# currdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# device = 'cuda'\n",
        "model.to('cuda')\n",
        "train_model(model,train_loader, val_loader, criterion, optimizer,scheduler, epochs)\n",
        "val_loss = evaluate_model(model, val_loader,criterion)\n",
        "print(f'Validation Loss:{val_loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yE6De1ipPdSn"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[43mmodel\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_full.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "torch.save(model, 'model_full.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVOrKs6EPo5T",
        "outputId": "f64dc6c8-d3c3-40e9-ffb5-d8e0c481602c"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_full.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "model = torch.load('model_full.pth')\n",
        "device = 'cpu'\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "BxJKX97vSs_x"
      },
      "outputs": [],
      "source": [
        "device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "F-_zP6jBSnsX"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, val_loader, criterion, device):\n",
        "    model.eval() \n",
        "    val_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():  \n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            outputs = outputs.view(-1, vocab_size)\n",
        "            labels = labels.view(-1)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    average_loss = val_loss / len(val_loader)\n",
        "    accuracy = (correct_predictions / total_samples) * 100\n",
        "    print(f'Validation Loss: {average_loss:.4f}, Accuracy: {accuracy:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ob7hkFpIQB-q"
      },
      "outputs": [],
      "source": [
        "# def predict(model, input_texts, tokenizer, device):\n",
        "#     model.eval()\n",
        "#     inputs = tokenizer(input_texts, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model(**inputs)\n",
        "#         # outputs = model(inputs['input_ids'], inputs['attention_mask'])\n",
        "#     return outputs\n",
        "\n",
        "def predict(model, input_texts, tokenizer, device):\n",
        "    model.eval()\n",
        "\n",
        "    encoded_inputs = tokenizer(input_texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "\n",
        "    input_ids = encoded_inputs['input_ids'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)  \n",
        "    return outputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBqLGHkPQWs_",
        "outputId": "68e09b07-a9cc-4d62-b5db-88fe244dac2a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m output_tensor \u001b[38;5;241m=\u001b[39m predict(\u001b[43mmodel\u001b[49m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwonderland\u001b[39m\u001b[38;5;124m\"\u001b[39m, tokenizer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m output_tensor\n",
            "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "output_tensor = predict(model,\"dfaf j sdfasf lkjlkjdf df\", tokenizer, 'cuda')\n",
        "output_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeE_Wv2mUvhm",
        "outputId": "ec9f978b-d5a9-4e24-e5a3-6aa53a71a8a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 30000])"
            ]
          },
          "execution_count": 162,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_tensor.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "KKMNsotPUXQV"
      },
      "outputs": [],
      "source": [
        "def process_language_model_output(output_tensor, tokenizer):\n",
        "    predicted_token_ids = torch.argmax(output_tensor, dim=-1)\n",
        "    decoded_texts = tokenizer.batch_decode(predicted_token_ids, skip_special_tokens=True)\n",
        "    return decoded_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFQisf00VMZF",
        "outputId": "22ed38e3-f7ce-432f-c3e2-3eaa140c0210"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['( she great d s very syp alice all what she whatd \" very only, arm whatd -']"
            ]
          },
          "execution_count": 164,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "process_language_model_output(output_tensor, tokenizer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
