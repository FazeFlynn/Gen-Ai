{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d--cuzlQ_Zaj",
        "outputId": "4d6b55ba-c86b-4c0b-c53e-cb4a40b89a30"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import AutoTokenizer\n",
        "import nltk\n",
        "# from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXGpzBRn_Zal",
        "outputId": "2bee4128-690c-404a-bdd6-5691da99e4af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"TITLE: Alice's Adventures in Wonderland\\n\",\n",
              " 'AUTHOR: Lewis Carroll\\n',\n",
              " '\\n',\n",
              " '\\n',\n",
              " '= CHAPTER I = \\n',\n",
              " '=( Down the Rabbit-Hole )=\\n',\n",
              " '\\n',\n",
              " '  Alice was beginning to get very tired of sitting by her sister\\n',\n",
              " 'on the bank, and of having nothing to do:  once or twice she had\\n',\n",
              " 'peeped into the book her sister was reading, but it had no\\n']"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# df = pd.read_csv('/content/alice_in_wonderland.txt')\n",
        "# mytext = df['question1'].tolist()\n",
        "# mytext[:5]\n",
        "\n",
        "text = ''\n",
        "\n",
        "with open('/content/alice_in_wonderland.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.readlines()\n",
        "\n",
        "text[:10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "J-5JHAHM_Zal"
      },
      "outputs": [],
      "source": [
        "max_seq_len = 512\n",
        "vocab_size = 30000\n",
        "embed_dim = 512\n",
        "num_layers = 6\n",
        "num_heads = 8\n",
        "hidden_dim = 1024\n",
        "batch_size = 16\n",
        "epochs = 10\n",
        "learning_rate = 1e-4\n",
        "warmup_steps = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "yAxPRQNM_Zam"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_len=max_seq_len):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        encoding = self.tokenizer.encode(text, max_length=self.max_len, padding='max_length', truncation=True)\n",
        "        return torch.tensor(encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "P0kjshaO_Zam"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_layers, num_heads, hidden_dim, max_seq_len):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.positional_encoding = self.get_positional_encoding(max_seq_len, embed_dim)\n",
        "\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
        "\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        device = x.device\n",
        "        positional_encoding = self.positional_encoding.to(device)\n",
        "        # x = self.embedding(x) + self.positional_encoding[:x.size(1), :]\n",
        "        x = self.embedding(x) + positional_encoding[:, :x.size(1), :]\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = self.fc(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "    def get_positional_encoding(self, max_seq_len, embed_dim):\n",
        "        pe = torch.zeros(max_seq_len, embed_dim)\n",
        "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-np.log(10000.0) / embed_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        return pe.unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc-KqHDN_Zam",
        "outputId": "87851211-43e3-4df5-eb73-0c622cae810a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "texts = text\n",
        "# tokenizer = word_tokenize\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "train_texts, val_texts = train_test_split(texts, test_size=0.2)\n",
        "\n",
        "train_dataset = TextDataset(train_texts, tokenizer)\n",
        "val_dataset = TextDataset(val_texts, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model = TransformerModel(vocab_size=vocab_size, embed_dim=embed_dim, num_layers=num_layers,\n",
        "                         num_heads=num_heads, hidden_dim=hidden_dim, max_seq_len=max_seq_len)\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=epochs*len(train_loader))\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "EeCDRThf_Zan"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch.long().to('cuda')\n",
        "            outputs = model(input_ids)\n",
        "\n",
        "            outputs = outputs.view(-1, vocab_size)\n",
        "            input_ids = input_ids.view(-1)\n",
        "            loss = criterion(outputs, input_ids)\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "G7aHX0SJ_Zao"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids =batch.long().to('cuda')\n",
        "            outputs = model(input_ids)\n",
        "\n",
        "            outputs =outputs.view(-1, vocab_size)\n",
        "            input_ids = input_ids.view(-1)\n",
        "            loss = criterion(outputs, input_ids)\n",
        "            total_loss +=loss.item()\n",
        "    return total_loss/len(val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3qUNLZJ_Zao",
        "outputId": "782c3163-145f-45e1-d1c1-ed2fc15f23f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 3.777030020621088\n",
            "Epoch 2/10, Loss: 0.1688496134347386\n",
            "Epoch 3/10, Loss: 0.09224488430966933\n",
            "Epoch 4/10, Loss: 0.05929569707562526\n",
            "Epoch 5/10, Loss: 0.04025054987933901\n",
            "Epoch 6/10, Loss: 0.028903769831069643\n",
            "Epoch 7/10, Loss: 0.021493608540751868\n",
            "Epoch 8/10, Loss: 0.017570449577437506\n",
            "Epoch 9/10, Loss: 0.015315832109707925\n",
            "Epoch 10/10, Loss: 0.014215607268528805\n",
            "Validation Loss:0.018477487978008057\n"
          ]
        }
      ],
      "source": [
        "# model.to('cpu')\n",
        "# currdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# device = 'cuda'\n",
        "model.to('cuda')\n",
        "train_model(model,train_loader, val_loader, criterion, optimizer,scheduler, epochs)\n",
        "val_loss = evaluate_model(model, val_loader,criterion)\n",
        "print(f'Validation Loss:{val_loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yE6De1ipPdSn"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[43mmodel\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_full.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "torch.save(model, 'model_full.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVOrKs6EPo5T",
        "outputId": "f64dc6c8-d3c3-40e9-ffb5-d8e0c481602c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4076\\207257105.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load('model_full.pth')\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "Can't get attribute 'TransformerModel' on <module '__main__'>",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_full.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:1097\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1095\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1096\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1097\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1105\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:1525\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# Needed for tensors where storage device and rebuild tensor device are\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# not connected (wrapper subclasses and tensors rebuilt using numpy)\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_thread_local_state\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[1;32m-> 1525\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_thread_local_state\u001b[38;5;241m.\u001b[39mmap_location\n\u001b[0;32m   1528\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:1515\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[1;34m(self, mod_name, name)\u001b[0m\n\u001b[0;32m   1513\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[1;32m-> 1515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mAttributeError\u001b[0m: Can't get attribute 'TransformerModel' on <module '__main__'>"
          ]
        }
      ],
      "source": [
        "model = torch.load('model_full.pth')\n",
        "device = 'cpu'\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "BxJKX97vSs_x"
      },
      "outputs": [],
      "source": [
        "device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "F-_zP6jBSnsX"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, val_loader, criterion, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            outputs = outputs.view(-1, vocab_size)\n",
        "            labels = labels.view(-1)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy (if classification)\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    average_loss = val_loss / len(val_loader)\n",
        "    accuracy = (correct_predictions / total_samples) * 100\n",
        "    print(f'Validation Loss: {average_loss:.4f}, Accuracy: {accuracy:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "ob7hkFpIQB-q"
      },
      "outputs": [],
      "source": [
        "# def predict(model, input_texts, tokenizer, device):\n",
        "#     model.eval()\n",
        "#     inputs = tokenizer(input_texts, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model(**inputs)\n",
        "#         # outputs = model(inputs['input_ids'], inputs['attention_mask'])\n",
        "#     return outputs\n",
        "\n",
        "def predict(model, input_texts, tokenizer, device):\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize and encode the input text\n",
        "    encoded_inputs = tokenizer(input_texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "\n",
        "    # Move tensors to the appropriate device\n",
        "    input_ids = encoded_inputs['input_ids'].to(device)\n",
        "\n",
        "    # Run inference\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)  # Call the model with a single argument\n",
        "    return outputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBqLGHkPQWs_",
        "outputId": "68e09b07-a9cc-4d62-b5db-88fe244dac2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[-14.0077, -18.0362, -17.8779,  ..., -17.0628, -18.8870, -18.0591],\n",
              "         [-10.1765, -10.6642, -10.2550,  ...,  -9.5856, -10.9056, -10.3538],\n",
              "         [ -6.1384, -10.6973,  -9.7426,  ..., -11.2020, -11.7542, -11.1571],\n",
              "         ...,\n",
              "         [ -8.1554, -10.8178, -10.9072,  ..., -10.6442, -10.8703, -10.8024],\n",
              "         [-10.1148, -10.6818, -10.4188,  ...,  -9.7317, -10.6712, -10.4630],\n",
              "         [-13.6406, -14.9259, -15.9283,  ..., -14.6627, -15.4674, -15.3328]]],\n",
              "       device='cuda:0')"
            ]
          },
          "execution_count": 161,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_tensor = predict(model,\"fdsff dfsdfi s j ffsd sdfdsf sfdgdfgg sdffwefd f\", tokenizer, 'cuda')\n",
        "output_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeE_Wv2mUvhm",
        "outputId": "ec9f978b-d5a9-4e24-e5a3-6aa53a71a8a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 30000])"
            ]
          },
          "execution_count": 162,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_tensor.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "KKMNsotPUXQV"
      },
      "outputs": [],
      "source": [
        "def process_language_model_output(output_tensor, tokenizer):\n",
        "    # Assuming output_tensor contains token IDs\n",
        "    predicted_token_ids = torch.argmax(output_tensor, dim=-1)\n",
        "    decoded_texts = tokenizer.batch_decode(predicted_token_ids, skip_special_tokens=True)\n",
        "    return decoded_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFQisf00VMZF",
        "outputId": "22ed38e3-f7ce-432f-c3e2-3eaa140c0210"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['( she great d s very syp alice all what she whatd \" very only, arm whatd -']"
            ]
          },
          "execution_count": 164,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "process_language_model_output(output_tensor, tokenizer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
